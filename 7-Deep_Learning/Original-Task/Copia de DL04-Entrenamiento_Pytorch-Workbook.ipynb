{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"DL04-Entrenamiento_Pytorch-Workbook.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Vw_LGfw4ZDtm"},"source":["# Deep Learining\n","# DL04 Entrenamiento Pytorch\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NZo0P8n_9OZB"},"source":["## <font color='blue'>Entrenando las redes neuronales</font>\n","\n","#### No ejecute el notebook completo. También tenga cuidado con el numero de epochs. El proceso puede ser lento. "]},{"cell_type":"markdown","metadata":{"id":"_Wt7PuKJUq_e"},"source":["### Función Loss o de perdida. \n","\n","La red que construimos en la parte 2, no sabe nada sobre nuestros dígitos escritos a mano. Sin embargo para que Las redes neuronales con activaciones no lineales funcionan como aproximadores de funciones universales, se debe enseñar la forma de la función. Que ocurre con las imagenes?, como le asignamos una función de aproximación o de aprendizje?. Las imágenes de dígitos escritos a mano la podemos asociar a probabilidades de clase. El poder de las redes neuronales es que podemos entrenarlas para aproximar esta función, y básicamente cualquier función con suficientes datos y tiempo de cálculo.\n","\n","![Log](https://drive.google.com/uc?export=view&id=1d7hNBU9q8x7D8XQPqDg1xqXuxrC9hyjS) \n","\n","\n","Al principio, la red es ingenua, no conoce la función que asigna las entradas a las salidas. Entrenamos la red mostrándole ejemplos de datos reales, luego ajustamos los parámetros de la red de modo que se aproxime a esta función.\n","\n","Para encontrar estos parámetros, necesitamos saber qué tan mal está prediciendo la red los resultados reales. Para esto calculamos una **función de pérdida (loss function)** (también llamada costo), una medida de nuestro error de predicción. Por ejemplo, la pérdida cuadrática media a menudo se usa en problemas de regresión y clasificación binaria.\n","$$\n","\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n","$$\n","\n","$$\n","\\large \\ell = -(ylog(p) + (1-y)  log(1-p))\n","$$\n","\n","Donde $n$ es el numero de ejemplo de entrenamiento, $y_i$ son los valores reales o etiquetas, y $\\hat{y}_i$ son los valores predecidos. "]},{"cell_type":"markdown","metadata":{"id":"UXuY-j6RU6rO"},"source":["### Optimización de los pesos \n","\n","\n","\n","\n","Al minimizar esta pérdida con respecto a los parámetros de la red, podemos encontrar configuraciones donde la pérdida es mínima y la red puede predecir las etiquetas correctas con alta precisión. Encontramos este mínimo usando un proceso llamado **descenso de gradiente**. El gradiente es la pendiente de la función de pérdida y apunta en la dirección del cambio más rápido. Para llegar al mínimo en la menor cantidad de tiempo, entonces queremos seguir el gradiente (hacia abajo). Puedes pensar en esto como descender una montaña siguiendo la pendiente más empinada hasta la base.\n","\n","\n","![Log](https://drive.google.com/uc?export=view&id=1-kPnRcfH8bhPH1vzQsWrRGC_0Nptc7jR) \n"]},{"cell_type":"markdown","metadata":{"id":"52Nz_60MZDtu"},"source":["### Backpropagation\n","\n","Para redes de una sola capa, el descenso de gradiente es fácil de implementar. Sin embargo, es más complicado para redes neuronales multicapa más profundas como la que hemos construido. Lo suficientemente complicado como para que pasaron unos 30 años antes de que los investigadores descubrieran cómo entrenar redes multicapa.\n","\n","El entrenamiento de redes multicapa se realiza a través de **backpropagation**, que en realidad es solo una aplicación de la regla de la cadena del cálculo. Es más fácil de entender si convertimos una red de dos capas en una representación gráfica.\n","\n","\n","![Log](https://drive.google.com/uc?export=view&id=1I7-fkRYztOZDU1P1JTo17DCFMc6O8tIO) \n","\n","\n","En el paso directo a través de la red, nuestros datos y operaciones van de abajo hacia arriba aquí. Pasamos la entrada $x$ a través de una transformación lineal $L_1$ con pesos  $W_1$ y sesgos $b_1$. La salida luego pasa por la operación sigmoidea $S$ y otra transformación lineal $L_2$. Finalmente calculamos la pérdida $ \\ell $. Usamos la pérdida como una medida de cuán malas son las predicciones de la red. El objetivo entonces es ajustar los pesos y sesgos para minimizar la pérdida.\n","\n","Para entrenar los pesos con descenso de gradiente, propagamos el gradiente de la pérdida hacia atrás a través de la red. Cada operación tiene algún gradiente entre las entradas y salidas. A medida que enviamos los gradientes hacia atrás, multiplicamos el gradiente entrante con el gradiente para la operación. Matemáticamente, esto es solo calcular el gradiente de la pérdida con respecto a los pesos usando la regla de la cadena.\n","\n","\n","$$\n","\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n","$$\n","\n","\n","Actualizamos nuestros pesos usando este gradiente con cierta tasa de aprendizaje $ \\ alpha $.\n","\n","$$\n","\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n","$$\n","\n","La tasa de aprendizaje $ \\ alpha $ se establece de manera que los pasos de actualización de peso sean lo suficientemente pequeños como para que el método iterativo se establezca en un mínimo."]},{"cell_type":"markdown","metadata":{"id":"1a7VJFyFZDtv"},"source":["### Losses in PyTorch\n","\n","Comencemos por ver cómo calculamos el loss con PyTorch. A través del módulo `nn`, PyTorch proporciona diferentes tipos de loss como por ejemplo ` nn.CrossEntropyLoss`.  En un problema de clasificación como MNIST, estamos utilizando la función softmax para predecir las probabilidades de clase. Con una salida softmax, desea utilizar la entropía cruzada como loss. Para calcular realmente el loss, primero define el criterio y luego pasa la salida de su red y las etiquetas correctas.\n","\n","\n","Para mas información mirar [the documentation for `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss),\n","\n","> Este criterio combina `nn.LogSoftmax()` y `nn.NLLLoss()` en una sola clase. \n",">\n"]},{"cell_type":"code","metadata":{"id":"h_LcMDmVZDtw"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","\n","# Normalizamos y transformamos. \n","\n","\n","transform = transforms.Compose([transforms.ToTensor(),\n","                              transforms.Normalize((0.5,), (0.5,)),\n","                              ])\n","# Bajamos y cargamos los datos. \n","trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UW_r-Y6fVeKt"},"source":["### Un primer ejemplo utlizando CrossEntropyLoss como loss "]},{"cell_type":"code","metadata":{"id":"qfpEdZ8jZDtw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315291,"user_tz":240,"elapsed":14,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"5d16ce63-e9a5-49eb-ffe5-9bcccc96ddd8"},"source":["# Contruimos una red  feed-forward.\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Linear(64, 10))\n","\n","# Definiendo el loss. Recordar que esta integrado con el softmax en el caso del CrossEntropyLoss. \n","criterion = nn.CrossEntropyLoss()\n","\n","# Obtengamos la data. \n","images, labels = next(iter(trainloader))\n","# Convirtiendo a vector.\n","images = images.view(images.shape[0], -1)\n","\n","# Forward pass, obteniendo el logits. \n","logits = model(images)\n","# Calculando el loss\n","loss = criterion(logits, labels)\n","\n","print(loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3071, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zjmm1gTzVlIH"},"source":["### Un segundoejemplo utlizando NLLLoss como loss \n","\n","https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html"]},{"cell_type":"code","metadata":{"id":"FKINvZdEZDty","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315292,"user_tz":240,"elapsed":12,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"eaab8808-0fe8-4937-ee7f-e41e551a6214"},"source":["# Contruimos una red  feed-forward.\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Linear(64, 10),\n","                      nn.LogSoftmax(dim=1))\n","\n","# Definiendo el loss. \n","criterion = nn.NLLLoss()\n","\n","# Obtengamos la data. \n","images, labels = next(iter(trainloader))\n","# Convirtiendo a vector.\n","images = images.view(images.shape[0], -1)\n","\n","# Forward pass, obteniendo las probabilidades. \n","logps = model(images)\n","# Calculando el loss\n","loss = criterion(logps, labels)\n","\n","print(loss)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3002, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"RjfVZ75GZDty"},"source":["### Autograd\n","\n","\n","Ahora que sabemos cómo calcular un loss, ¿cómo lo usamos para realizar la propagación hacia atrás? Torch proporciona un módulo, 'autograd', para calcular automáticamente los gradientes de los tensores. Podemos usarlo para calcular los gradientes de todos nuestros parámetros con respecto a la pérdida. Autograd funciona haciendo un seguimiento de las operaciones realizadas en los tensores, luego retrocediendo a través de esas operaciones, calculando los gradientes en el camino. Para asegurarse de que PyTorch realiza un seguimiento de las operaciones en un tensor y calcula los gradientes, debe establecer `require_grad = True` en un tensor. Puede hacer esto en la creación con la palabra clave `require_grad`, o en cualquier momento con` x.requires_grad_ (True) `.\n","\n","Puede desactivar gradientes para un bloque de código con el `torch.no_grad()`:\n","```python\n","x = torch.zeros(1, requires_grad=True)\n",">>> with torch.no_grad():\n","...     y = x * 2\n",">>> y.requires_grad\n","False\n","```\n","Tambien podemos apagar el calculo de gradientes con `torch.set_grad_enabled(True|False)`.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4MY9_O_hW3iU"},"source":["Los gradientes son caculados respecto de alguna variable `z` con `z.backward()`."]},{"cell_type":"markdown","metadata":{"id":"GDnb3C61W61g"},"source":["#### Un ejemplo concreto con calculos de gradientes."]},{"cell_type":"code","metadata":{"id":"uuECWBvjZDtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315292,"user_tz":240,"elapsed":9,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"1f8929ea-2b03-41f5-9489-5b93ceb6875b"},"source":["x = torch.randn(2,2, requires_grad=True)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.6470,  0.8587],\n","        [ 0.0163,  1.1565]], requires_grad=True)\n"]}]},{"cell_type":"code","metadata":{"id":"EZMba81FZDtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315619,"user_tz":240,"elapsed":333,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"60ec10c6-a59b-48b9-f526-8db995162ac8"},"source":["y = x**2\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[4.1866e-01, 7.3744e-01],\n","        [2.6451e-04, 1.3374e+00]], grad_fn=<PowBackward0>)\n"]}]},{"cell_type":"code","source":["m = x + y\n","m"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkVR7N7AglGE","executionInfo":{"status":"ok","timestamp":1650502315619,"user_tz":240,"elapsed":22,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"690b7da4-b750-4e75-d931-70302f337749"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.2284,  1.5962],\n","        [ 0.0165,  2.4938]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"gQZU71QkZDt0"},"source":["A continuación podemos ver la operación que creó `y`, una operación de potencia`PowBackward0`."]},{"cell_type":"code","metadata":{"id":"5Gh9a1MzZDt0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315619,"user_tz":240,"elapsed":21,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"3d8deaf0-9a2e-46ed-8e48-a82f76aa8bdc"},"source":["## grad_fn muestra la funcion generada por esta variable. \n","print(y.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<PowBackward0 object at 0x7f5b8a8b5450>\n"]}]},{"cell_type":"markdown","metadata":{"id":"U6X5EUofZDt0"},"source":["El módulo de autograd realiza un seguimiento de estas operaciones y sabe cómo calcular el gradiente de cada una. De esta manera, puede calcular los gradientes para una cadena de operaciones, con respecto a cualquier tensor. Reduzcamos el tensor `y` a un valor escalar, la media."]},{"cell_type":"code","metadata":{"id":"TU52UEFiZDt0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315620,"user_tz":240,"elapsed":20,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"b006eee8-542b-40b9-9b6b-faae2a2c78f3"},"source":["z = y.mean()\n","print(z)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.6234, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"joPJE4q0ZDt1"},"source":["Puede verificar los gradientes para `x` e` y` pero actualmente están vacíos."]},{"cell_type":"code","metadata":{"id":"JXgZGgFGZDt1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315620,"user_tz":240,"elapsed":18,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"5f892267-dbeb-4052-bccf-1c0534e0b22d"},"source":["print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"markdown","metadata":{"id":"wilFOvRCZDt1"},"source":["Para calcular los gradientes, debe ejecutar el método `.backward` en una Variable,` z` por ejemplo. Esto calculará el gradiente para `z` con respecto a` x`\n","\n","$$\n","\\frac{\\partial z}{\\partial x_j} = \\frac{\\partial}{\\partial x_j}\\left[\\frac{1}{4}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n","$$"]},{"cell_type":"code","metadata":{"id":"JCxXKPMIZDt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315620,"user_tz":240,"elapsed":16,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"946cef97-73e1-43c0-919b-96c1e746fc68"},"source":["z.backward()\n","print(x.grad)\n","print(x/2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.3235,  0.4294],\n","        [ 0.0081,  0.5782]])\n","tensor([[-0.3235,  0.4294],\n","        [ 0.0081,  0.5782]], grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"4iHbvJgvZDt2"},"source":["### Loss and Autograd juntos\n","\n","\n","Cuando creamos una red con PyTorch, todos los parámetros se inicializan con `require_grad = True`. Esto significa que cuando calculamos el loss y llamamos `loss.backward ()`, se calculan los gradientes para los parámetros. Estos gradientes se utilizan para actualizar los pesos con descenso de gradiente. "]},{"cell_type":"code","metadata":{"id":"8PUFjD0nZDt2"},"source":["# Construimos una red  feed-forward.\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Dropout(0.2),\n","                      nn.Linear(64, 10),\n","                      nn.Dropout(0.2),\n","                      nn.LogSoftmax(dim=1))\n","\n","criterion = nn.NLLLoss()\n","images, labels = next(iter(trainloader))\n","images = images.view(images.shape[0], -1)\n","\n","logps = model(images)\n","loss = criterion(logps, labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxA496CuiCQJ","executionInfo":{"status":"ok","timestamp":1650502315621,"user_tz":240,"elapsed":14,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"9689cbd3-571f-40f4-fe41-14aa2174cb49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3298, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"TzjusbaXZDt2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650502315621,"user_tz":240,"elapsed":12,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"30c75dc0-9ad6-4b64-db49-30d71e579b71"},"source":["print('Before backward pass: \\n', model[0].weight.grad)\n","\n","loss.backward()\n","\n","print('After backward pass: \\n', model[0].weight.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before backward pass: \n"," None\n","After backward pass: \n"," tensor([[ 5.3710e-04,  5.3710e-04,  5.3710e-04,  ...,  5.3710e-04,\n","          5.3710e-04,  5.3710e-04],\n","        [-9.7758e-05, -9.7758e-05, -9.7758e-05,  ..., -9.7758e-05,\n","         -9.7758e-05, -9.7758e-05],\n","        [-1.1543e-03, -1.1543e-03, -1.1543e-03,  ..., -1.1543e-03,\n","         -1.1543e-03, -1.1543e-03],\n","        ...,\n","        [-1.4343e-03, -1.4343e-03, -1.4343e-03,  ..., -1.4343e-03,\n","         -1.4343e-03, -1.4343e-03],\n","        [-7.6849e-03, -7.6849e-03, -7.6849e-03,  ..., -7.6849e-03,\n","         -7.6849e-03, -7.6849e-03],\n","        [ 4.5016e-03,  4.5016e-03,  4.5016e-03,  ...,  4.5016e-03,\n","          4.5016e-03,  4.5016e-03]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ECQjpT4vZDt2"},"source":["### Entrenemos la red. \n","\n","Hay una última pieza que necesitamos para comenzar a entrenar, un optimizador que usaremos para actualizar los pesos con los gradientes. Los obtenemos del paquete [`optim` package](https://pytorch.org/docs/stable/optim.html). Por ejemplo, podemos usar el descenso de gradiente estocástico con `optim.SGD`. Se puede ver cómo definir un optimizador a continuación."]},{"cell_type":"code","source":["next(iter(model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAsf4T6ylkwu","executionInfo":{"status":"ok","timestamp":1650502315621,"user_tz":240,"elapsed":10,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"50347010-c04e-4125-ef84-1f9aa6871c8d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0347, -0.0147, -0.0302,  ...,  0.0045,  0.0339, -0.0312],\n","        [ 0.0015, -0.0083,  0.0160,  ...,  0.0092, -0.0037,  0.0036],\n","        [-0.0007,  0.0081,  0.0092,  ...,  0.0112,  0.0020,  0.0064],\n","        ...,\n","        [-0.0001, -0.0278,  0.0262,  ..., -0.0279, -0.0338, -0.0250],\n","        [-0.0044,  0.0230, -0.0154,  ..., -0.0324, -0.0242, -0.0279],\n","        [ 0.0012,  0.0053,  0.0054,  ...,  0.0098,  0.0079, -0.0200]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["print(\"Antes de la actualización de pesos\")\n","model[0].weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzZCevRxlr7u","executionInfo":{"status":"ok","timestamp":1650502315621,"user_tz":240,"elapsed":8,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"ad0e3091-e798-4acf-ce16-f2266f37bcc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Antes de la actualización de pesos\n"]},{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0347, -0.0147, -0.0302,  ...,  0.0045,  0.0339, -0.0312],\n","        [ 0.0015, -0.0083,  0.0160,  ...,  0.0092, -0.0037,  0.0036],\n","        [-0.0007,  0.0081,  0.0092,  ...,  0.0112,  0.0020,  0.0064],\n","        ...,\n","        [-0.0001, -0.0278,  0.0262,  ..., -0.0279, -0.0338, -0.0250],\n","        [-0.0044,  0.0230, -0.0154,  ..., -0.0324, -0.0242, -0.0279],\n","        [ 0.0012,  0.0053,  0.0054,  ...,  0.0098,  0.0079, -0.0200]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"kaRYwZYxZDt3"},"source":["from torch import optim\n","\n","# Los optimizadores requieren los parámetros para optimizar y una tasa de aprendizaje\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer.step()"],"metadata":{"id":"n_RhCZp0mIMJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Después de la actualización de pesos\")\n","model[0].weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbVLOuufmRch","executionInfo":{"status":"ok","timestamp":1650502315871,"user_tz":240,"elapsed":6,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"244ae764-1eaf-4cfc-e80d-3d806ead220f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Después de la actualización de pesos\n"]},{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0347, -0.0147, -0.0302,  ...,  0.0045,  0.0339, -0.0312],\n","        [ 0.0015, -0.0083,  0.0160,  ...,  0.0092, -0.0037,  0.0036],\n","        [-0.0007,  0.0081,  0.0092,  ...,  0.0113,  0.0020,  0.0064],\n","        ...,\n","        [-0.0001, -0.0278,  0.0263,  ..., -0.0279, -0.0338, -0.0249],\n","        [-0.0043,  0.0231, -0.0153,  ..., -0.0323, -0.0241, -0.0278],\n","        [ 0.0012,  0.0052,  0.0054,  ...,  0.0097,  0.0079, -0.0201]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["``` python\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #generamos un iterador utilizando pytorch\n","\n","optimizer = optim.SGD(model.parameters(), lr = 0.01)\n","\n","for inputs, labels in iterador: # el iterador será el train loader (trainset dividido en muestras pequeñas)\n","  outputs = model(inputs) # forward pass para output de nuestra red neuronal, probabilidades\n","  loss = criterion(outputs, labels) # cómputo función de pérdida\n","  loss.backward() # Backpropagation\n","  optimizer.step() # actualización\n","```"],"metadata":{"id":"pgh8AMr2muqr"}},{"cell_type":"markdown","metadata":{"id":"YW-p8u1uqIWs"},"source":["## <font color='green'>**Actividad 4**</font>\n","\n","Ahora sabemos cómo usar todas las partes individuales, así que es momento de ver cómo funcionan juntas.  El proceso general con PyTorch:\n","\n","\n","* Hacer un forward a través de la red\n","* Use la salida de red para calcular el loss\n","* Realice un paso hacia atrás a través de la red con `loss.backward ()` para calcular los gradientes\n","* Da un paso con el optimizador para actualizar los pesos\n","\n","### Entrenamiento.\n","\n","Ahora pondremos este algoritmo en un ciclo para que podamos ver todas las imágenes. Alguna nomenclatura, una pasada a través de todo el conjunto de datos se llama **epoch**. Así que aquí vamos a recorrer el \"trainloader\" para obtener nuestros lotes de entrenamiento. Para cada lote, haremos un pase de entrenamiento donde calcularemos la pérdida, haremos un pase hacia atrás y actualizaremos los pesos."]},{"cell_type":"markdown","metadata":{"id":"fPUEL24xpleS"},"source":["### Tenga cuidado con el numero de epochs. El proceso puede ser lento. \n","\n","**Ayuda**\n","\n","1. Defina una red por ejemplo:\n","\n","\n","\n","```\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Linear(64, 10),\n","                      nn.LogSoftmax(dim=1))\n","```\n","2. Defina el loss\n","\n","\n","\n","```\n","criterion = nn.NLLLoss()\n","```\n","\n","\n","3. Defina el optimizador\n","\n","\n","\n","```\n","optimizer = optim.SGD(model.parameters(), lr=0.003)\n","```\n","\n","\n","4. Itere respecto de las epochs:\n","\n","\n","\n","```\n","for e in range(epochs):\n","    running_loss = 0\n","    for images, labels in trainloader:\n","    # Genere un vector de 784 caracteristicas. \n","    images = images.view(images.shape[0], -1)\n","    #Realice el forward\n","    # Calcule el loss.\n","    # Realice el backward()\n","    # Optimice\n","    # Acumule el loss.\n","```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"FsZNqsGyZDt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650505568599,"user_tz":240,"elapsed":110112,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"6a349be3-6a37-442a-f70d-55106f190b21"},"source":["trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","# Definimos la red\n","model = nn.Sequential(nn.Linear(784, 256),\n","                      nn.ReLU(),\n","                      nn.Linear(256, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 64),\n","                      nn.ReLU(),\n","                      nn.Linear(64, 10),\n","                      nn.LogSoftmax(dim=1))\n","\n","# función loss negative log-likelihood\n","criterion = nn.NLLLoss()\n","\n","# Optimizador\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# Definimos epochs, vector loss\n","epochs = 7\n","vloss = []\n","\n","for e in range(epochs):\n","    running_loss = 0\n","    for inputs, labels in trainloader:\n","      inputs = inputs.view(inputs.shape[0], -1)\n","      outputs = model(inputs) \n","      loss = criterion(outputs, labels)\n","      loss.backward() \n","      optimizer.step()\n","      running_loss += loss.item()\n","    vloss.append(running_loss)\n","    print(f\"Training Loss: {running_loss}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Loss: 1884.578199148178\n","Training Loss: 2371.332386493683\n","Training Loss: 2380.3013155460358\n","Training Loss: 2456.270655632019\n","Training Loss: 2535.9719541072845\n","Training Loss: 2551.2229177951813\n","Training Loss: 2562.1855244636536\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(range(epochs), vloss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"oMBcw3loxNuC","executionInfo":{"status":"ok","timestamp":1650505590435,"user_tz":240,"elapsed":226,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"8c37ca9a-aae5-4c2d-8c93-26a2c3ca83b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f5b86f7c390>]"]},"metadata":{},"execution_count":81},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"7YcxpL6PZDt3"},"source":["Con la red entrenada, podemos verificar sus predicciones. Utilice las siguientes funciones. \n"]},{"cell_type":"code","metadata":{"id":"HF9WiE7AqlzB"},"source":["import numpy as np\n","\n","def view_classify(img, ps, version=\"MNIST\"):\n","    ''' Function for viewing an image and it's predicted classes.\n","    '''\n","    ps = ps.data.numpy().squeeze()\n","\n","    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n","    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n","    ax1.axis('off')\n","    ax2.barh(np.arange(10), ps)\n","    ax2.set_aspect(0.1)\n","    ax2.set_yticks(np.arange(10))\n","    if version == \"MNIST\":\n","        ax2.set_yticklabels(np.arange(10))\n","    elif version == \"Fashion\":\n","        ax2.set_yticklabels(['T-shirt/top',\n","                            'Trouser',\n","                            'Pullover',\n","                            'Dress',\n","                            'Coat',\n","                            'Sandal',\n","                            'Shirt',\n","                            'Sneaker',\n","                            'Bag',\n","                            'Ankle Boot'], size='small');\n","    ax2.set_title('Class Probability')\n","    ax2.set_xlim(0, 1.1)\n","\n","    plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-W2K84HqZDt3","colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"status":"ok","timestamp":1650505597173,"user_tz":240,"elapsed":675,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"691a00e8-dd93-45db-83c6-635b482f6a53"},"source":["%matplotlib inline\n","\n","images, labels = next(iter(trainloader))\n","\n","img = images[0].view(1, 784)\n","# Aqui sirve apagar los gradientes para ser mas eficientes. \n","with torch.no_grad():\n","    logps = model(img) # Este es el modelo\n","\n","# La salida de la red son logaritmos de probabilidades (Por la función de perdida utilizada), \n","# tenemos que aplicar una exponencial para obtener probabilidades. \n","ps = torch.exp(logps)\n","view_classify(img.view(1, 28, 28), ps)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x648 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"kSo9YGHIZDt4"},"source":["Ahora nuestra red puede predecir de mejor precisión los dígitos en nuestras imágenes."]},{"cell_type":"markdown","metadata":{"id":"p1ZhCvJ9qQXe"},"source":["<font color='green'>**Fin Actividad 4**</font>"]}]}