{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"DL02-Tensores_Pytorch.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"E0cL0d-bWGtN"},"source":["# Deep Learning\n","# DL02 Tensores en Pytorch"]},{"cell_type":"markdown","metadata":{"id":"5EcooyFb6iHg"},"source":["\n","\n","En este notebook, presentamos  [PyTorch] (http://pytorch.org/), un framework para construir y entrenar redes neuronales. En el calculo de redes neuronales se utilizan frecuentemente matrices y de forma mas general tensores.  PyTorch toma estos tensores y hace que sea simple moverlos a GPU para el procesamiento más rápido necesario al entrenar redes neuronales. También proporciona un módulo que calcula automáticamente los gradientes (¡para propagación hacia atrás!) Y otro módulo específicamente para construir redes neuronales. "]},{"cell_type":"markdown","metadata":{"id":"hQWso2ebWGtU"},"source":["## <font color='blue'>**Redes Neuronales.**</font>\n","\n","Deep Learning se basa en redes neuronales artificiales que han existido de alguna forma desde finales de la década de 1950. Las redes se construyen a partir de partes individuales que se aproximan a las neuronas, típicamente llamadas unidades o simplemente \"neuronas\". Cada unidad tiene un cierto número de entradas ponderadas. Estas entradas ponderadas se suman (una combinación lineal) y luego se pasan a través de una función de activación para obtener la salida de la unidad.\n","\n","![Log](https://drive.google.com/uc?export=view&id=1EBHN-Ho1ZmYoRy1x2ZkER9fLWBSTwvO3) \n","\n","\n","Matematicamente esto se ve de la siguiente forma:\n","\n","$$\n","\\begin{align}\n","y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n","y &= f\\left(\\sum_i w_i x_i +b \\right)\n","\\end{align}\n","$$\n","\n","Si lo expereamos en notacion vectorial esto es basicamente un prodcuto interno entre dos vectores.\n","\n","$$\n","h = \\begin{bmatrix}\n","x_1 \\, x_2 \\cdots  x_n\n","\\end{bmatrix}\n","\\cdot \n","\\begin{bmatrix}\n","           w_1 \\\\\n","           w_2 \\\\\n","           \\vdots \\\\\n","           w_n\n","\\end{bmatrix}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"dNKIE1wrWGtU"},"source":["### Tensores la base de los calculos en redes neuronales. \n","\n","Resulta que los cálculos de la red neuronal son solo un montón de operaciones de álgebra lineal de **tensores** (una generalización de las matrices). Un vector es un tensor unidimensional, una matriz es un tensor bidimensional, una matriz con tres índices es un tensor tridimensional (imágenes de color RGB, por ejemplo). La estructura de datos fundamental para las redes neuronales son los tensores y PyTorch (así como casi cualquier otro framework de aprendizaje profundo) se construye alrededor de los tensores.\n","\n","\n","![Log](https://drive.google.com/uc?export=view&id=1p_zNdbAbwDPCk4ZvZ_2U7MklfkW8OCr1) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"wSRYaFr2Xbvd"},"source":["\n","### Exploremos cómo podemos usar PyTorch para construir una red neuronal simple."]},{"cell_type":"code","metadata":{"id":"Va2B9K3KWGtV"},"source":["# Primero importemos PyTorch\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_TaP8cY8WGtV"},"source":["# Construyamos \n","def activation(x):\n","    \"\"\" Sigmoid activation function \n","    \n","        Arguments\n","        ---------\n","        x: torch.Tensor\n","    \"\"\"\n","    return 1/(1+torch.exp(-x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXL49j2jWGtW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650322370057,"user_tz":240,"elapsed":1123,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"cfdc3d15-d71d-4fe3-9ffa-4f3f60528992"},"source":["### Generemos datos.\n","torch.manual_seed(7) # Definamos una semilla para poder reproducir el calculo.\n","\n","# Features son 5 variables normales aleatorias. \n","features = torch.randn((1, 5))\n","print(features)\n","# Retorna pesos utilizando una distribución normal con media 0 y variana 1.\n","weights = torch.randn_like(features)\n","print(weights)\n","#  Adicionalmente definimos un bias. \n","bias = torch.randn((1, 1))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n","tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Gh5yLBbJWGtW"},"source":["Arriba se generaron  datos que podemos usar para obtener la salida de nuestra red simple. Todo esto es aleatorio por ahora, en adelante comenzaremos a usar datos normales. Analisemos cada linea:\n","\n","`features = torch.randn((1, 5))` crea un tensor de forma `(1, 5)`, 1 fila y 5 columnas, este vector contiene valores aleatoriamente distribuidos de acuerdo a una distribución normal con media 0 y desviación estandar 1.\n","\n","`weights = torch.randn_like(features)` crea otro tensor con la misma forma que `features`, y nuevamente conteniendo valores de una distribución normal. \n","\n","Finalmente , `bias = torch.randn((1, 1))` crea un unico valor obtenido de una distribución normal. \n","\n","Los tensores PyTorch se pueden agregar, multiplicar, restar, etc., al igual que las matrices de Numpy. En general, usará los tensores PyTorch de la misma manera que usaría las matrices Numpy. Sin embargo, vienen con algunos buenos beneficios, como la aceleración de GPU, que veremos más adelante. Por ahora, use los datos generados para calcular la salida de esta red simple de una sola capa.\n","\n","\n","**Ejemplo**: Calcule la salida de la redo considerando como input a `features`, como pesos `weights`, y bias `bias`. PyTorch tiene un metodo [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum), para tomar sumas. Adicionalmente utilice la función `activation` definida  anteriormente como función de activación. "]},{"cell_type":"code","metadata":{"id":"qoZY7-EzUT88"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9qSiEOwWGtW","executionInfo":{"status":"ok","timestamp":1650322370059,"user_tz":240,"elapsed":17,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"632d80a9-a7f9-490c-c595-92b0df3fe20f"},"source":["### Solucion\n","# Sol 1\n","y = activation(torch.sum(features * weights) + bias)\n","print(y)\n","# Sol 2\n","y = activation((features * weights).sum() + bias)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1595]])\n","tensor([[0.1595]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"e_MpegDAWGtX"},"source":["Sin embargo, puedes hacer la multiplicación y la suma en la misma operación usando una multiplicación matricial. En general, querrás usar multiplicaciones matriciales ya que son más eficientes y aceleradas usando bibliotecas modernas y computación de alto rendimiento en GPU.\n","\n","Aquí, queremos hacer una multiplicación matricial de las features y los weights. Para esto podemos usar [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul)\n","\n","```python\n",">> torch.mm(features, weights)\n","\n","---------------------------------------------------------------------------\n","RuntimeError                              Traceback (most recent call last)\n","<ipython-input-13-15d592eb5279> in <module>()\n","----> 1 torch.mm(features, weights)\n","\n","RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n","```\n","A medida que construye redes neuronales en cualquier framework, este error aparecerá a menudo. Lo que sucede aquí es que nuestros tensores no son las formas correctas para realizar una multiplicación matricial. Recuerde que para las multiplicaciones matriciales, El número de columnas en el primer tensor debe ser igual al número de filas en la segunda columna. Ambas `features` y` weights` tienen la misma forma, `(1, 5)`. Esto significa que necesitamos cambiar la forma de los 'pesos' para que la multiplicación de la matriz funcione.\n","\n","\n","**Nota:** Para ver la forma de un tensor `tensor`, usamos `tensor.shape`. \n","\n","Ecisten opciones para cambiar la forma a un tensor: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), and [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n","\n","* `weights.reshape(a, b)` Retornará un tensor con la misma data que `weights` y con tamaño `(a, b)`. \n","* `weights.resize_(a, b)` Retorna un tensor con distinta forma. Si la nueva forma tiene menos elementos que la original, algunos seran removidos. Si la nueva forma tiene mas elementos que el original, los nuevos elementos serán no inicializados.   [read more about in-place operations](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) in PyTorch.\n","* `weights.view(a, b)` retornará el mismo tensor con la misma data que  `weights` con tamaño `(a, b)`.\n","\n","> **Ejemplo**: Calcule la salida de nuestra pequeña red usando la multiplicación de matrices."]},{"cell_type":"code","metadata":{"id":"foxGeXdDWGtY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650322370060,"user_tz":240,"elapsed":13,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"71b92039-5e65-4dba-f3f2-973ffb96848c"},"source":["## Solucion\n","\n","y = activation(torch.mm(features, weights.view(5,1)) + bias)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1595]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"H33DCWT4WGtY"},"source":["### ¡Apilarlos!\n","\n","Así es como puede calcular la salida de una sola neurona. El poder real de este algoritmo ocurre cuando comienzas a apilar estas unidades individuales en capas y pilas de capas, en una red de neuronas. La salida de una capa de neuronas se convierte en la entrada para la siguiente capa. Con múltiples unidades de entrada y unidades de salida, ahora necesitamos expresar los pesos como una matriz.\n","\n","![Log](https://drive.google.com/uc?export=view&id=1baAB8q9xxML3osQFAHfWdQQPzpMTEf-G) \n","\n","\n","La primera capa que se muestra en la parte inferior aquí son las entradas, llamadas ** capa de entrada **. La capa intermedia se llama ** capa oculta **, y la capa final (a la derecha) es la ** capa de salida **. Podemos expresar esta red matemáticamente con matrices nuevamente y usar la multiplicación de matrices para obtener combinaciones lineales para cada unidad en una operación. Por ejemplo, la capa oculta ($ h_1 $ y $ h_2 $ aquí) se puede calcular\n","\n","$$\n","\\vec{h} = [h_1 \\, h_2] = \n","\\begin{bmatrix}\n","x_1 \\, x_2 \\cdots \\, x_n\n","\\end{bmatrix}\n","\\cdot \n","\\begin{bmatrix}\n","           w_{11} & w_{12} \\\\\n","           w_{21} &w_{22} \\\\\n","           \\vdots &\\vdots \\\\\n","           w_{n1} &w_{n2}\n","\\end{bmatrix}\n","$$\n","\n","La salida para esta pequeña red se encuentra tratando la capa oculta como entradas para la unidad de salida. La salida de la red se expresa simplemente\n","\n","\n","$$\n","y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Tvle7MhabcmF"},"source":["## <font color='green'>**Actividad 1**</font>\n","\n","1. Defina una semilla para reproducir el calculo\n","\n","2. Genere un dataset  con tres variables aleatorias.\n","\n","3. Defina el tensor de pesos aleatorios entre la capa de entrada y la capa hidden. \n","\n","4. Defina un tensor de pesos aleatorios entre la capa hidden y la de salida.\n","\n","5. Defina los tensores bias para la capa hidden y de salida.\n","\n","6. Calcule la salida para esta red multicapa utilizando los pesos `W1` y` W2`, y los sesgos, `B1` y` B2`.\n","\n","\n","\n","\n","```\n","torch.mm(features, W1)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"CEAuTo6hWGtY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650323107112,"user_tz":240,"elapsed":444,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}},"outputId":"9e530229-2364-479c-a8b3-d316e44ed626"},"source":["### Construya los tensores y un dataset de prueba.\n","torch.manual_seed(7) # Asignemos una semilla para poder reproducir el calculo. "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f1b8d3daed0>"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"uKLbpGWuWGta","outputId":"69c8647c-e563-4eb1-d321-6e1c6d23e2ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650323943627,"user_tz":240,"elapsed":492,"user":{"displayName":"MARIO ANTONIO CASTILLO MACHUCA","userId":"11911169784446403315"}}},"source":["### Enlace las distintas capas. Utilice torch.mm para realizar la multiplicación de tensores. \n","\"\"\"\n","Si fijó la semilla en 7 el resultado debiese ser:\n","tensor([[0.6813, 0.4355]])\n","tensor([[0.3171]])\n","\"\"\"\n","import torch.nn.functional as F\n","\n","# setting seed\n","torch.manual_seed(7)\n","\n","# generating some random features\n","features = torch.randn(1, 3) \n","\n","# define the weights\n","W1 = torch.randn((3, 2), requires_grad=True)\n","W2 = torch.randn((2, ), requires_grad=True)\n","\n","# define the bias terms\n","B1 = torch.randn((2), requires_grad=True)\n","B2 = torch.randn((), requires_grad=True)\n","\n","# calculate hidden and output layers\n","h1 = activation(torch.mm(features, W1) + B1)\n","output = torch.sigmoid((h1 @ W2) + B2)\n","\n","print(f\"Atributos:\\n{features}\\n\\n\")\n","print(f\"Vector de pesos capa de entrada:\\n{W1}\\n\\n\")\n","print(f\"Vector de pesos capa oculta:\\n{W2}\\n\\n\")\n","print(f\"Vector de bias para capa oculta:\\n{B1}\\n\\n\")\n","print(f\"Vector de bias para capa de salida:\\n{B2}\\n\\n\")\n","print(f\"Vector de outputs capa oculta:\\n{h1}\")\n","print(f\"Resultado final:\\n{output}\\n\\n\")"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Atributos:\n","tensor([[-0.1468,  0.7861,  0.9468]])\n","\n","\n","Vector de pesos capa de entrada:\n","tensor([[-1.1143,  1.6908],\n","        [-0.8948, -0.3556],\n","        [ 1.2324,  0.1382]], requires_grad=True)\n","\n","\n","Vector de pesos capa oculta:\n","tensor([-1.6822,  0.3177], requires_grad=True)\n","\n","\n","Vector de bias para capa oculta:\n","tensor([0.1328, 0.1373], requires_grad=True)\n","\n","\n","Vector de bias para capa de salida:\n","0.2405461221933365\n","\n","\n","Vector de outputs capa oculta:\n","tensor([[0.6813, 0.4355]], grad_fn=<MulBackward0>)\n","Resultado final:\n","tensor([0.3171], grad_fn=<SigmoidBackward0>)\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"EP9emaIjdJyR"},"source":["<font color='green'>**Fin Actividad 1**</font>"]},{"cell_type":"markdown","metadata":{"id":"wUGblR3IWGta"},"source":["Si hizo esto correctamente, debería ver la salida `tensor ([[0.3171]])`.\n","\n","El número de unidades ocultas de un parámetro de la red, a menudo llamado **hiperparámetro** para diferenciarlo de los parámetros de weight y bias. Como verá más adelante cuando analicemos cómo entrenar una red neuronal, cuantas más unidades ocultas tenga una red y más capas, mejor podrá aprender de los datos y hacer predicciones precisas."]}]}